.\"-
.\" SPDX-License-Identifier: BSD-3-Clause
.\"
.\" Copyright (c) 2019-2020, Intel Corporation
.\" All rights reserved.
.\"
.\" Redistribution and use in source and binary forms of the Software, with or
.\" without modification, are permitted provided that the following conditions
.\" are met:
.\" 1. Redistributions of source code must retain the above copyright notice,
.\"    this list of conditions and the following disclaimer.
.\"
.\" 2. Redistributions in binary form must reproduce the above copyright notice,
.\"    this list of conditions and the following disclaimer in the documentation
.\"    and/or other materials provided with the distribution.
.\"
.\" 3. Neither the name of the Intel Corporation nor the names of its
.\"    contributors may be used to endorse or promote products derived from
.\"    this Software without specific prior written permission.
.\"
.\" THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
.\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
.\" IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
.\" ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
.\" LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
.\" CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
.\" SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
.\" INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
.\" CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
.\" ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
.\" POSSIBILITY OF SUCH DAMAGE.
.\"
.\" * Other names and brands may be claimed as the property of others.
.\"
.Dd May 20, 2024
.Dt ICE 4
.Os
.Sh NAME
.Nm ice
.Nd "Intel Ethernet 800 Series Driver"
.Sh SYNOPSIS
To compile this driver into the kernel, place the following lines in your
kernel configuration file:
.Bd -ragged -offset indent
.Cd "device iflib"
.Cd "device ice"
.Ed
.Pp
To load the driver as a module at boot time, place the following lines in
.Xr loader.conf 5 :
.Bd -literal -offset indent
if_ice_load="YES"
.Ed
.Sh DESCRIPTION
.Ss Features
The
.Nm
driver provides support for any PCI Express adapter or LOM
(LAN On Motherboard)
in the Intel Ethernet 800 Series.
As of this writing, the series includes devices with these model numbers:
.Pp
.Bl -bullet -compact
.It
Intel\(rg Ethernet Controller E810\-C
.It
Intel\(rg Ethernet Controller E810\-XXV
.It
Intel\(rg Ethernet Connection E822\-C
.It
Intel\(rg Ethernet Connection E822\-L
.It
Intel\(rg Ethernet Connection E823\-C
.It
Intel\(rg Ethernet Connection E823\-L
.It
Intel\(rg Ethernet Connection E825\-C
.It
Intel\(rg Ethernet Connection E830\-C
.It
Intel\(rg Ethernet Connection E830\-CC
.It
Intel\(rg Ethernet Connection E830\-L
.It
Intel\(rg Ethernet Connection E830\-XXV
.El
.Pp
For questions related to hardware requirements, refer to the documentation
supplied with your adapter.
.Pp
Support for Jumbo Frames is provided via the interface MTU setting.
Selecting an MTU larger than 1500 bytes with the
.Xr ifconfig 8
utility configures the adapter to receive and transmit Jumbo Frames.
The maximum MTU size for Jumbo Frames is 9706.
This value coincides with the maximum Jumbo Frame size of 9728.
.Pp
This driver version supports VLANs.
For information on enabling VLANs, see the
.Pa README .
.Pp
Offloads are also controlled via the interface, for instance, checksumming for
both IPv4 and IPv6 can be set and unset, TSO4 and/or TSO6, and finally LRO can
be set and unset.
.Pp
For more information on configuring this device, see
.Xr ifconfig 8 .
.Pp
The associated Virtual Function (VF) driver for this driver is
.Xr iavf 4 .
.Pp
The associated RDMA driver for this driver is
.Xr irdma 4 .
.Ss Dynamic Device Personalization
The DDP package loads during device initialization.
The driver looks for the
.Em ice_ddp
module and checks that it contains a valid DDP package file.
.Pp
If the driver is unable to load the DDP package, the device will enter Safe
Mode.
Safe Mode disables advanced and performance features and supports only
basic traffic and minimal functionality, such as updating the NVM or
downloading a new driver or DDP package.
Safe Mode only applies to the affected physical function and does not impact
any other PFs.
See the
.Em Intel(R) Ethernet Adapters and Devices User Guide
for more details on DDP and Safe Mode.
.Pp
If you encounter issues with the DDP package file, you may need to download
an updated driver or
.Em ice_ddp
module. See the log messages for more information.
.Pp
You cannot update the DDP package if any PF drivers are already loaded.
To overwrite a package, unload all PFs and then reload the driver with the
new package.
.Pp
You can only use one DDP package per driver, even if you have more than one
device installed that uses the driver.
.Pp
Only the first loaded PF per device can download a package for that device.
.Ss Remote Direct Memory Access
.sp
Remote Direct Memory Access, or RDMA, allows a network device to transfer data
directly to and from application memory on another system, increasing
throughput and lowering latency in certain networking environments.
.Pp
The ice driver supports both the iWARP (Internet Wide Area RDMA Protocol) and
RoCEv2 (RDMA over Converged Ethernet) protocols.
The major difference is that iWARP performs RDMA over TCP, while RoCEv2 uses
UDP.
.Pp
Devices based on the Intel(R) Ethernet 800 Series do not support RDMA when
operating in multiport mode with more than 4 ports.
.Pp
For detailed installation and configuration information for RDMA, see
.Xr irdma 4 .
.Ss RDMA Monitoring
For debugging/testing purposes, you can use sysctl to set up a mirroring
interface on a port.
The interface can receive mirrored RDMA traffic for packet
analysis tools like
.Xr tcpdump 1 .
This mirroring may impact performance.
.Pp
To use RDMA monitoring, you may need to reserve more MSI\-X interrupts.
Before the
.Nm
driver loads, configure the following tunable:
.Bd -literal -offset indent
dev.ice.0.iflib.use_extra_msix_vectors=4
.Ed
.Pp
You may need to adjust the number of extra MSI\-X interrupt vectors.
.Pp
To create/delete the interface:
.Bd -literal -offset indent
sysctl dev.ice.0.create_interface=1
sysctl dev.ice.0.delete_interface=1
.Ed
.Pp
The mirrored interface receives both LAN and RDMA traffic. Additional
filters can be configured in tcpdump.
.Pp
To differentiate the mirrored interface from the primary interface, the network
interface naming convention is:
.Bd -literal -offset indent
<driver name><port number><modifier><modifier unit number>
.Ed
.Pp
For example,
.Dq Li ice0m0
is the first mirroring interface on
.Dq Li ice0 .
.Ss Data Center Bridging
Data Center Bridging (DCB) is a configuration Quality of Service
implementation in hardware.
It uses the VLAN priority tag (802.1p) to filter traffic.
That means that there are 8 different priorities that traffic can be filtered
into.
It also enables priority flow control (802.1Qbb) which can limit or eliminate
the number of dropped packets during network stress.
Bandwidth can be allocated to each of these priorities, which is enforced at
the hardware level (802.1Qaz).
.Pp
DCB is normally configured on the network using the DCBX protocol (802.1Qaz), a
specialization of LLDP (802.1AB). The
.Nm
driver supports the following mutually exclusive variants of DCBX support:
.Bl -bullet -compact
.It
Firmware\-based LLDP Agent
.It
Software\-based LLDP Agent
.El
.Pp
In firmware\-based mode, firmware intercepts all LLDP traffic and handles DCBX
negotiation transparently for the user.
In this mode, the adapter operates in
.Dq willing
DCBX mode, receiving DCB settings from the link partner (typically a
switch).
The local user can only query the negotiated DCB configuration.
For information on configuring DCBX parameters on a switch, please consult the
switch manufacturer\(aqs documentation.
.Pp
In software\-based mode, LLDP traffic is forwarded to the network stack and user
space, where a software agent can handle it.
In this mode, the adapter can operate in
.Dq nonwilling
DCBX mode and DCB configuration can be both queried and set locally.
This mode requires the FW\-based LLDP Agent to be disabled.
.Pp
Firmware\-based mode and software\-based mode are controlled by the
.Dq fw_lldp_agent
sysctl.
Refer to the Firmware Link Layer Discovery Protocol Agent section for more
information.
.Pp
Link\-level flow control and priority flow control are mutually exclusive.
The ice driver will disable link flow control when priority flow control
is enabled on any traffic class (TC).
It will disable priority flow control when link flow control is enabled.
.Pp
To enable/disable priority flow control in software\-based DCBX mode:
.Bd -literal -offset indent
sysctl dev.ice.<interface #>.pfc=1 (or 0 to disable)
.Ed
Enhanced Transmission Selection (ETS) allows you to assign bandwidth to certain
TCs, to help ensure traffic reliability.
To view the assigned ETS configuration, use the following:
.Bd -literal -offset indent
sysctl dev.ice.<interface #>.ets_min_rate
.Ed
To set the minimum ETS bandwidth per TC, separate the values by commas.
All values must add up to 100.
For example, to set all TCs to a minimum bandwidth of 10% and TC 7 to 30%,
use the following:
.Bd -literal -offset indent
sysctl dev.ice.<interface #>.ets_min_rate=10,10,10,10,10,10,10,30
.Ed
To set the User Priority (UP) to a TC mapping for a port, separate the values
by commas. For example, to map UP 0 and 1 to TC 0, UP 2 and 3 to TC 1, UP 4 and
5 to TC 2, and UP 6 and 7 to TC 3, use the following:
.Bd -literal -offset indent
sysctl dev.ice.<interface #>.up2tc_map=0,0,1,1,2,2,3,3
.Ed
.Ss L3 QoS mode
The
.Nm
driver supports setting DSCP\-based Layer 3 Quality of Service (L3 QoS)
in the PF driver.
The driver initializes in L2 QoS mode by default; L3 QoS is disabled by
default.
Use the following sysctl to enable or disable L3 QoS:
.Bd -literal -offset indent
sysctl dev.ice.<interface #>.pfc_mode=1 (or 0 to disable)
.Ed
If you disable L3 QoS mode, it returns to L2 QoS mode.
.Pp
To map a DSCP value to a traffic class, separate the values by commas. For
example, to map DSCPs 0\-3 and DSCP 8 to DCB TCs 0\-3 and 4, respectively:
.Bd -literal -offset indent
sysctl dev.ice.<interface #>.dscp2tc_map.0\-7=0,1,2,3,0,0,0,0
sysctl dev.ice.<interface #>.dscp2tc_map.8\-15=4,0,0,0,0,0,0,0
.Ed
.Ss Additional Utilities
There are additional tools available from Intel to help configure and update
the adapters covered by this driver.
These tools can be downloaded directly from Intel at
.Lk https://downloadcenter.intel.com ,
by searching for their names:
.Bl -bullet
.It
To change the behavior of the QSFP28 ports on E810-C adapters, use the Intel
.Em Ethernet Port Configuration Tool - FreeBSD .
.It
To update the firmware on an adapter, use the Intel
.Em Non-Volatile Memory (NVM) Update Utility for Intel Ethernet Network Adapters E810 series - FreeBSD
.El
.Sh HARDWARE
The
.Nm
driver supports the Intel Ethernet 800 series.
Most adapters in this series with SFP28/QSFP28 cages
have firmware that requires that Intel qualified modules are used; these
qualified modules are listed below.
This qualification check cannot be disabled by the driver.
.Pp
The
.Nm
driver supports 100Gb Ethernet adapters with these QSFP28 modules:
.Pp
.Bl -bullet -compact
.It
Intel\(rg 100G QSFP28 100GBASE-SR4   E100GQSFPSR28SRX
.It
Intel\(rg 100G QSFP28 100GBASE-SR4   SPTMBP1PMCDF
.It
Intel\(rg 100G QSFP28 100GBASE-CWDM4 SPTSBP3CLCCO
.It
Intel\(rg 100G QSFP28 100GBASE-DR    SPTSLP2SLCDF
.El
.Pp
The
.Nm
driver supports 25Gb and 10Gb Ethernet adapters with these SFP28 modules:
.Pp
.Bl -bullet -compact
.It
Intel\(rg 10G/25G SFP28 25GBASE-SR E25GSFP28SR
.It
Intel\(rg     25G SFP28 25GBASE-SR E25GSFP28SRX (Extended Temp)
.It
Intel\(rg     25G SFP28 25GBASE-LR E25GSFP28LRX (Extended Temp)
.El
.Pp
The
.Nm
driver supports 10Gb and 1Gb Ethernet adapters with these SFP+ modules:
.Pp
.Bl -bullet -compact
.It
Intel\(rg 1G/10G SFP+ 10GBASE-SR E10GSFPSR
.It
Intel\(rg 1G/10G SFP+ 10GBASE-SR E10GSFPSRG1P5
.It
Intel\(rg 1G/10G SFP+ 10GBASE-SR E10GSFPSRG2P5
.It
Intel\(rg    10G SFP+ 10GBASE-SR E10GSFPSRX (Extended Temp)
.It
Intel\(rg 1G/10G SFP+ 10GBASE-LR E10GSFPLR
.El
.Pp
Note that adapters also support all passive and active
limiting direct attach cables that comply with SFF-8431 v4.1 and
SFF-8472 v10.4 specifications.
.Pp
This is not an exhaustive list; please consult product documentation for an
up-to-date list of supported media.
.Sh LOADER TUNABLES
Tunables can be set at the
.Xr loader 8
prompt before booting the kernel or stored in
.Xr loader.conf 5 .
See the
.Xr iflib 4
man page for more information on using iflib sysctl variables as tunables.
.Bl -tag -width indent
.It Va hw.ice.enable_health_events
Set to 1 to enable firmware health event reporting across all devices. Enabled by default.
.Pp
If enabled, when the driver receives a firmware health event message, it will
print out a description of the event to the kernel message buffer and if
applicable, possible actions to take to remedy it.
.It Va hw.ice.irdma
Set to 1 to enable the RDMA client interface, required by the
.Xr irdma 4
driver.
Enabled by default.
.It Va hw.ice.rdma_max_msix
Set the maximum number of per-device MSI-X vectors that are allocated for use by the
.Xr irdma 4
driver.
Set to 64 by default.
.It Va hw.ice.debug.enable_tx_fc_filter
Set to 1 to enable the TX Flow Control filter across all devices. Enabled by default.
.Pp
If enabled, the hardware will drop any transmitted Ethertype 0x8808 control
frames that do not originate from the hardware.
.It Va hw.ice.debug.enable_tx_lldp_filter
Set to 1 to enable the TX LLDP filter across all devices. Enabled by default.
.Pp
If enabled, the hardware will drop any transmitted Ethertype 0x88cc LLDP frames
that do not originate from the hardware.
This must be disabled in order to use LLDP daemon software such as
.Xr lldpd 8 .
.It Va hw.ice.debug.ice_tx_balance_en
Set to 1 to allow the driver to use the 5-layer Tx Scheduler tree topology if configured by the DDP package.
.Pp
Enabled by default. 
.El
.Sh SYSCTL PROCEDURES
.Bl -tag -width indent
.It Va dev.ice.#.fc
Allows one to set the flow control value.
A value of 0 disables flow control, 3 enables full, 1 is RX, and 2 is
TX pause.
.It Va dev.ice.#.advertise_speed
Allows one to set advertised link speeds, this will then cause a link
renegotiation.
.It Va dev.ice.#.current_speed
This is a display of the current setting.
.It Va dev.ice.#.fw_version
Displays the current firmware and NVM versions of the adapter.
.It Va dev.ice.#.ddp_version
TBW
.It Va dev.ice.#.requested_fec
TBW
.It Va dev.ice.#.negotiated_fec
TBW
.It Va dev.ice.#.fw_lldp_agent
TBW
.It Va dev.ice.#.ets_min_rate
TBW
.It Va dev.ice.#.up2tc_map
TBW
.It Va dev.ice.#.pfc
TBW
.El
.Sh INTERRUPT STORMS
It is important to note that 100G operation can generate high
numbers of interrupts, often incorrectly being interpreted as
a storm condition in the kernel.
It is suggested that this be resolved by setting
.Va hw.intr_storm_threshold
to 0.
.Sh SUPPORT
For general information and support,
go to the Intel support website at:
.Lk http://www.intel.com/support/ .
.Pp
If an issue is identified with this driver with a supported adapter,
email all the specific information related to the issue to
.Aq Mt freebsd@intel.com .
.Sh SEE ALSO
.Xr arp 4 ,
.Xr iflib 4 ,
.Xr netintro 4 ,
.Xr ng_ether 4 ,
.Xr vlan 4 ,
.Xr ifconfig 8
.Sh HISTORY
The
.Nm
device driver first appeared in
.Fx 12.2 .
.Sh AUTHORS
The
.Nm
driver was written by
.An Intel Corporation Aq Mt freebsd@intel.com .
